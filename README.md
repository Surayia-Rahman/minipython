This is a bunch of university small projects catering to different aspects. The description are as below:

First and foremost, the FLP assignment (Functional and Logical Programming):

   This project simulates the creation and analysis of a system log file. It performs the following key tasks:

 a) Log File Generation:
 
A fake log file is automatically created with 1000 entries. Each entry includes a timestamp, log level (INFO, ERROR, WARNING, or DEBUG), and a message based on the log level.

 b) Data Extraction and Processing:
 
The log file is read and parsed to extract timestamps, log levels, and message content. The script calculates:

  - The number of log entries per hour (24-hour breakdown)

  - The number of entries for each log level

  - The average length of log messages

 c) Data Visualization:
 
The extracted data is visualized using bar charts, a pie chart, and a histogram:

  - Log activity by hour

  - Distribution of log levels

  - Distribution of message lengths

Result Summary:

At the end, the average length of the log messages is printed for quick insight into message verbosity.

‚úÖ Functional Programming Perspective

1) The project simulates and analyzes log data using a functional programming mindset by emphasizing:

2) Pure Functions: Each function (e.g., generating messages, extracting info) performs a specific task with predictable outputs and no side effects.

3) Data Transformation: The program processes data through mapping and transformations, rather than altering it in-place.

4) Immutability: Variables like log entries and counts are created fresh at each stage, without mutating global state.

5) Composability: Small reusable functions are composed together to form the full workflow‚Äîfrom reading logs to generating statistics and plotting.

Overall, the program demonstrates clean, declarative, and stateless design, which aligns with functional programming principles.

üîç Logical Programming Perspective

From a logical programming point of view, the project could be viewed as:

1) A collection of facts: Each log line (timestamp, log level, message) represents a structured fact in the system.

2) Rule-based reasoning: One could define rules to infer patterns, such as detecting peak log hours or high error rates.

3) Declarative intent: Instead of detailing how to calculate stats, you can express what conditions or patterns you're interested in (e.g., ‚ÄúFind hours with more than 50 warnings‚Äù).

4) Inference and querying: The data could be queried for specific conditions or used to derive insights through logical relationships.

Thus, in the logical paradigm, the project becomes about defining relationships and extracting knowledge from structured facts.


   The second one we'll be talking about is the mlp model for regression task. This code demonstrates how to use a Multilayer Perceptron (MLP) Regressor for regression tasks, a machine learning technique commonly used for approximating complex functions.

1) Data Generation:

Synthetic data is generated by creating random values for the feature X (between 0 and 5), and the target values y are calculated as the sine of X with some added noise. This introduces variability, simulating real-world data.

2) Model Creation:

An MLP model is created using the MLPRegressor class from scikit-learn. The model has two hidden layers, one with 100 neurons and the other with 50 neurons, which is typically used for learning non-linear relationships in data. The model is configured to run for a maximum of 1000 iterations to ensure convergence.

3) Model Training:

The model is trained by fitting it to the generated data (X, y). During training, the model adjusts its internal weights to minimize the error between the predicted outputs and actual target values (y).

4) Prediction:

After training, the model is used to predict values (y_pred) for new input data points (x_new), ranging from 0.0 to 5.0.

5) Visualization:

The training data points (X, y) and the model's predicted regression line (y_pred) are plotted on a graph. This helps visualize how well the model has learned the underlying sine function despite the added noise.

Key Points:

The MLPRegressor is a neural network-based regression model that can learn complex patterns in data.

The model is trained using a dataset that mimics a noisy sine wave, and after training, it can predict continuous values for new inputs.

The result is visualized with a plot showing both the actual noisy data and the regression line fitted by the model.

In conclusion, this code showcases the use of a neural network for regression to approximate a non-linear function, and its effectiveness can be evaluated by the accuracy of the fitted line compared to the true underlying function (sine wave).

Using this notion, the sample stock price dataset analysis can be referred to.

The quiz game is just for fun :)

     The last one would be using scapy on a surface level for a little bit of experiment. 

This script performs a network scan to discover devices connected to a local network and retrieves information about those devices, including their IP address, MAC address, and vendor name. Here's a breakdown of how it works:

1. get_vendor(mac_address) function:
This function takes a MAC address as input and uses an external API (macvendors.com) to retrieve the vendor name associated with that MAC address.

It sends an HTTP GET request to the https://api.macvendors.com/{mac_address} endpoint and returns the vendor name if the request is successful (HTTP status 200).

If an error occurs (e.g., the API is unavailable or the request fails), the function returns "Unknown".

2. scan_network(target_ip_range, timeout) function:
This function scans the local network for devices by sending ARP (Address Resolution Protocol) requests to the specified IP range.

The IP range to scan is provided in CIDR notation (default is "192.168.100.1/24", which means it scans all IP addresses in the 192.168.100.1 - 192.168.100.254 range).

ARP is used to map IP addresses to MAC addresses. The function sends an ARP request for each device in the IP range, and the devices that are active on the network will respond with their MAC addresses.

The srp function from the Scapy library is used to send and receive ARP packets, with a specified timeout (timeout=2 seconds).

After receiving the responses, the function calls the get_vendor function to fetch the vendor name for each MAC address and stores the results in a list of dictionaries. Each dictionary contains the following information:

a. IP address of the device (received.psrc)

b. MAC address of the device (received.hwsrc)

c. Vendor name retrieved using the get_vendor function

3. Example usage:
   
The script then calls scan_network() to perform the scan and prints the results.

It outputs the discovered devices in a tabular format, listing each device's IP address, MAC address, and vendor name (if available).


